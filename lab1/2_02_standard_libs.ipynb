{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec9bdea",
   "metadata": {},
   "source": [
    "**Задача 1: Система управления библиотекой**\n",
    "\n",
    "Создайте систему управления библиотекой используя collections:\n",
    "\n",
    "1. Создайте namedtuple `Book` с полями: title, author, isbn, year\n",
    "2. Создайте namedtuple `Reader` с полями: name, reader_id, phone\n",
    "3. Используйте `defaultdict(list)` для хранения книг по жанрам\n",
    "4. Используйте `deque` для очереди читателей, ожидающих популярную книгу\n",
    "5. Используйте `Counter` для подсчета количества книг каждого автора\n",
    "6. Используйте `OrderedDict` для хранения истории выдачи книг (читатель -> список книг)\n",
    "7. Сериализуйте все данные в JSON и pickle форматы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bccfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque, Counter, OrderedDict\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# 1) + 2)\n",
    "Book = namedtuple('Book', ['title', 'author', 'isbn', 'year'])\n",
    "Reader = namedtuple('Reader', ['name', 'reader_id', 'phone'])\n",
    "\n",
    "# 3) Create and fill defaultdict with books\n",
    "books_dict = defaultdict(list)\n",
    "books_dict['scifi'].append(Book(\"fant1\", \"author1\", \"111\", 2000))\n",
    "books_dict['scifi'].append(Book(\"fant2\", \"author2\", \"222\", 2000))\n",
    "books_dict['drama'].append(Book(\"drama1\", \"author3\", \"333\", 2000))\n",
    "books_dict['drama'].append(Book(\"drama2\", \"author4\", \"444\", 2000))\n",
    "books_dict['action'].append(Book(\"act1\", \"author2\", \"555\", 2000))\n",
    "books_dict['action'].append(Book(\"act2\", \"author3\", \"666\", 2000))\n",
    "\n",
    "# 4) Create and fill deque with readers\n",
    "readers : deque[namedtuple] = deque()\n",
    "readers.append(Reader(\"name1\", \"11\", \"384\"))\n",
    "readers.append(Reader(\"name2\", \"22\", \"384\"))\n",
    "readers.append(Reader(\"name3\", \"33\", \"384\"))\n",
    "readers.append(Reader(\"name4\", \"44\", \"384\"))\n",
    "\n",
    "# 5) Counting authors\n",
    "cnt = Counter()\n",
    "for genre_books in books_dict.values():\n",
    "    for book in genre_books:\n",
    "        cnt[book.author] += 1\n",
    "\n",
    "# 6)Create and fill OrderedDict\n",
    "book_history : OrderedDict[namedtuple, namedtuple] = OrderedDict()\n",
    "book_history[readers[0]] = books_dict[\"scifi\"][0]\n",
    "book_history[readers[1]] = books_dict[\"drama\"][0]\n",
    "book_history[readers[2]] = books_dict[\"action\"][0]\n",
    "\n",
    "# 7) Serialize into JSON\n",
    "json_data = {\n",
    "    \"books_by_genre\": {genre: [_._asdict() for _ in genre_books] for genre, genre_books in books_dict.items()},\n",
    "    \"readers_queue\": [r._asdict() for r in readers],\n",
    "    \"author_counts\": dict(cnt),\n",
    "    \"book_history\": [\n",
    "        {\"reader\": {\"name\": r.name, \"reader_id\": r.reader_id, \"phone\": r.phone},\n",
    "         \"book\": b._asdict()}\n",
    "        for r, b in book_history.items()\n",
    "    ],\n",
    "}\n",
    "\n",
    "with open('library_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "# Serialize into pickle\n",
    "with open('library_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        \"books_dict\": books_dict,\n",
    "        \"readers\": readers,\n",
    "        \"author_counts\": cnt,\n",
    "        \"book_history\": book_history,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3d9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'books_by_genre': {'scifi': [{'title': 'fant1',\n",
       "     'author': 'author1',\n",
       "     'isbn': '111',\n",
       "     'year': 2000},\n",
       "    {'title': 'fant2', 'author': 'author2', 'isbn': '222', 'year': 2000}],\n",
       "   'drama': [{'title': 'drama1',\n",
       "     'author': 'author3',\n",
       "     'isbn': '333',\n",
       "     'year': 2000},\n",
       "    {'title': 'drama2', 'author': 'author4', 'isbn': '444', 'year': 2000}],\n",
       "   'action': [{'title': 'act1',\n",
       "     'author': 'author2',\n",
       "     'isbn': '555',\n",
       "     'year': 2000},\n",
       "    {'title': 'act2', 'author': 'author3', 'isbn': '666', 'year': 2000}]},\n",
       "  'readers_queue': [{'name': 'name1', 'reader_id': '11', 'phone': '384'},\n",
       "   {'name': 'name2', 'reader_id': '22', 'phone': '384'},\n",
       "   {'name': 'name3', 'reader_id': '33', 'phone': '384'},\n",
       "   {'name': 'name4', 'reader_id': '44', 'phone': '384'}],\n",
       "  'author_counts': {'author1': 1, 'author2': 2, 'author3': 2, 'author4': 1},\n",
       "  'book_history': [{'reader': {'name': 'name1',\n",
       "     'reader_id': '11',\n",
       "     'phone': '384'},\n",
       "    'book': {'title': 'fant1',\n",
       "     'author': 'author1',\n",
       "     'isbn': '111',\n",
       "     'year': 2000}},\n",
       "   {'reader': {'name': 'name2', 'reader_id': '22', 'phone': '384'},\n",
       "    'book': {'title': 'drama1',\n",
       "     'author': 'author3',\n",
       "     'isbn': '333',\n",
       "     'year': 2000}},\n",
       "   {'reader': {'name': 'name3', 'reader_id': '33', 'phone': '384'},\n",
       "    'book': {'title': 'act1',\n",
       "     'author': 'author2',\n",
       "     'isbn': '555',\n",
       "     'year': 2000}}]},\n",
       " {'books_dict': defaultdict(list,\n",
       "              {'scifi': [Book(title='fant1', author='author1', isbn='111', year=2000),\n",
       "                Book(title='fant2', author='author2', isbn='222', year=2000)],\n",
       "               'drama': [Book(title='drama1', author='author3', isbn='333', year=2000),\n",
       "                Book(title='drama2', author='author4', isbn='444', year=2000)],\n",
       "               'action': [Book(title='act1', author='author2', isbn='555', year=2000),\n",
       "                Book(title='act2', author='author3', isbn='666', year=2000)]}),\n",
       "  'readers': deque([Reader(name='name1', reader_id='11', phone='384'),\n",
       "         Reader(name='name2', reader_id='22', phone='384'),\n",
       "         Reader(name='name3', reader_id='33', phone='384'),\n",
       "         Reader(name='name4', reader_id='44', phone='384')]),\n",
       "  'author_counts': Counter({'author2': 2,\n",
       "           'author3': 2,\n",
       "           'author1': 1,\n",
       "           'author4': 1}),\n",
       "  'book_history': OrderedDict([(Reader(name='name1', reader_id='11', phone='384'),\n",
       "                Book(title='fant1', author='author1', isbn='111', year=2000)),\n",
       "               (Reader(name='name2', reader_id='22', phone='384'),\n",
       "                Book(title='drama1', author='author3', isbn='333', year=2000)),\n",
       "               (Reader(name='name3', reader_id='33', phone='384'),\n",
       "                Book(title='act1', author='author2', isbn='555', year=2000))])})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################### ADDITIONAL TASK TO DO #########################\n",
    "### Read and recreate saved file\n",
    "\n",
    "# Read and recreate from JSON\n",
    "with open('library_data.json', 'r', encoding='utf-8') as f:\n",
    "    loaded_json = json.load(f)\n",
    "\n",
    "# Recreate books_dict from JSON\n",
    "books_dict_loaded = defaultdict(list)\n",
    "for genre, books in loaded_json[\"books_by_genre\"].items():\n",
    "    books_dict_loaded[genre] = [Book(**b) for b in books]\n",
    "\n",
    "# Recreate readers deque from JSON\n",
    "readers_loaded = deque([Reader(**r) for r in loaded_json[\"readers_queue\"]])\n",
    "\n",
    "# Recreate author Counter from JSON\n",
    "author_counts_loaded = Counter(loaded_json[\"author_counts\"])\n",
    "\n",
    "# Recreate book_history OrderedDict from JSON\n",
    "book_history_loaded = OrderedDict()\n",
    "for entry in loaded_json[\"book_history\"]:\n",
    "    reader = Reader(**entry[\"reader\"])\n",
    "    book = Book(**entry[\"book\"])\n",
    "    book_history_loaded[reader] = book\n",
    "\n",
    "# Read and recreate from pickle\n",
    "with open('library_data.pkl', 'rb') as f:\n",
    "    pickle_data = pickle.load(f)\n",
    "\n",
    "books_dict_from_pickle = pickle_data[\"books_dict\"]\n",
    "readers_from_pickle = pickle_data[\"readers\"]\n",
    "author_counts_from_pickle = pickle_data[\"author_counts\"]\n",
    "book_history_from_pickle = pickle_data[\"book_history\"]\n",
    "\n",
    "loaded_json, pickle_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79281705",
   "metadata": {},
   "source": [
    "**Задача 2: Анализатор файловой системы**\n",
    "\n",
    "Создайте анализатор файловой системы используя os и sys:\n",
    "\n",
    "1. Создайте namedtuple `FileInfo` с полями: name, size, extension, modified_time\n",
    "2. Используйте `os.walk()` для обхода директории\n",
    "3. Используйте `os.path` функции для получения информации о файлах\n",
    "4. Используйте `Counter` для подсчета файлов по расширениям\n",
    "5. Используйте `defaultdict(list)` для группировки файлов по размеру (маленькие < 1MB, средние 1-100MB, большие > 100MB)\n",
    "6. Используйте `deque` для хранения последних 10 найденных файлов\n",
    "7. Выведите статистику используя `sys.getsizeof()` для подсчета памяти\n",
    "8. Сохраните результаты в JSON файл\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1041bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter, deque, namedtuple\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# 0) Initialize data structures for 2), 5), 6)\n",
    "extension_counter : Counter = Counter()\n",
    "files_by_size : defaultdict = defaultdict(list)\n",
    "last_10_files : deque = deque(maxlen=10)\n",
    "\n",
    "# 1)\n",
    "FileInfo : namedtuple = namedtuple('FileInfo', ['name', 'size', 'extension', 'modified_time'])\n",
    "\n",
    "# 2) Scan directory with os.walk()\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        \n",
    "        # 3) Get file information (using os.path)\n",
    "        file_size = os.path.getsize(filepath)\n",
    "        file_ext = os.path.splitext(filename)[1].lower()\n",
    "        mod_time = os.path.getmtime(filepath)\n",
    "        \n",
    "        # Create FileInfo object\n",
    "        file_info = FileInfo(\n",
    "            name=filename,\n",
    "            size=file_size,\n",
    "            extension=file_ext,\n",
    "            modified_time=mod_time\n",
    "        )\n",
    "        \n",
    "        # 4) Count files by extension (using Counter lib)\n",
    "        extension_counter[file_ext] += 1\n",
    "\n",
    "        # 5) Sort files by size (using defaultdict)\n",
    "        if file_size < 1024 ** 2:   # < 1MB\n",
    "            files_by_size['small'].append(file_info)\n",
    "        elif file_size < 100 * 1024 * 1024:   # 1-100MB\n",
    "            files_by_size['medium'].append(file_info)\n",
    "        else:   # > 100MB\n",
    "            files_by_size['large'].append(file_info)\n",
    "        \n",
    "        # 6) Add to deque for last 10 files\n",
    "        last_10_files.append(file_info)\n",
    "\n",
    "# 7) Calculate memory usage (using .getsizeof())\n",
    "memory_usage = {\n",
    "    'extension_counter': sys.getsizeof(extension_counter),\n",
    "    'files_by_size': sys.getsizeof(files_by_size),\n",
    "    'last_10_files': sys.getsizeof(last_10_files),\n",
    "    'total': sys.getsizeof(extension_counter) + sys.getsizeof(files_by_size) + sys.getsizeof(last_10_files)\n",
    "}\n",
    "\n",
    "# Print statistics and memory usage (using .getsizeof())\n",
    "print(\"Files by extension:\", dict(extension_counter))\n",
    "print(\"Files by size:\", {k: len(v) for k, v in files_by_size.items()})\n",
    "print(\"Last 10 files:\", [f.name for f in last_10_files])\n",
    "print(\"Memory usage:\", memory_usage)\n",
    "\n",
    "# 8) Serialize to JSON\n",
    "json_data = {\n",
    "    'files_by_extension': dict(extension_counter),\n",
    "    'files_by_size_category': {\n",
    "        category: [f._asdict() for f in files] \n",
    "        for category, files in files_by_size.items()\n",
    "    },\n",
    "    'last_10_files': [f._asdict() for f in last_10_files],\n",
    "    'memory_usage': memory_usage\n",
    "}\n",
    "\n",
    "with open('filesystem_analysis.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3aa3f4",
   "metadata": {},
   "source": [
    "**Задача 3: Система конфигурации приложения**\n",
    "\n",
    "Создайте систему конфигурации используя ChainMap и defaultdict:\n",
    "\n",
    "1. Создайте namedtuple `Config` с полями: key, value, section, default_value\n",
    "2. Создайте несколько словарей конфигурации (default, user, environment)\n",
    "3. Используйте `ChainMap` для объединения конфигураций с приоритетом\n",
    "4. Используйте `defaultdict(dict)` для группировки настроек по секциям\n",
    "5. Используйте `OrderedDict` для сохранения порядка загрузки конфигураций\n",
    "6. Используйте `os.environ` для чтения переменных окружения\n",
    "7. Сериализуйте конфигурацию в JSON и pickle форматы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c6e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "localhost\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, ChainMap, defaultdict, OrderedDict\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# 1) Config namedtuple\n",
    "Config = namedtuple('Config', ['key', 'value', 'section', 'default_value'])\n",
    "\n",
    "# 2) Create configuration dicts\n",
    "default_config = {\n",
    "    'port': 8000,\n",
    "    'host': 'localhost'\n",
    "}\n",
    "\n",
    "user_config = {\n",
    "    'port': 3000,\n",
    "    'host': 'localhost'\n",
    "}\n",
    "\n",
    "# 6) Take some environment (port, host) variables using os.environ\n",
    "env_config = {}\n",
    "if 'PORT' in os.environ:\n",
    "    env_config['port'] = int(os.environ['PORT'])\n",
    "if 'HOST' in os.environ:\n",
    "    env_config['host'] = os.environ['HOST']\n",
    "\n",
    "# 3) ChainMap\n",
    "config_chain = ChainMap(env_config, user_config, default_config)\n",
    "\n",
    "# 4) Group configuration. Combine them by key 'main'\n",
    "grouped_config = defaultdict(dict)\n",
    "for key, value in config_chain.items():\n",
    "    grouped_config['main'][key] = value\n",
    "\n",
    "# 5) Create OrderedDict (default -> user -> environment) \n",
    "config_order = OrderedDict()\n",
    "config_order['default'] = default_config\n",
    "config_order['user'] = user_config\n",
    "config_order['environment'] = env_config\n",
    "\n",
    "# 7) Prepare list[dict] for serialization\n",
    "config_entries = []\n",
    "for key, final_value in config_chain.items():\n",
    "    config_entries.append(Config(\n",
    "        key=key,\n",
    "        value=final_value,\n",
    "        section='main',\n",
    "    ))\n",
    "\n",
    "# JSON serialization\n",
    "json_data = {\n",
    "    'final_config': dict(config_chain),\n",
    "    'grouped_config': dict(grouped_config),\n",
    "    'config_order': dict(config_order),\n",
    "    'config_entries': [c._asdict() for c in config_entries]\n",
    "}\n",
    "\n",
    "with open('config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "# Pickle serialization\n",
    "with open('config.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'chain': config_chain,\n",
    "        'grouped': grouped_config,\n",
    "        'order': config_order,\n",
    "        'entries': config_entries\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b9c21",
   "metadata": {},
   "source": [
    "**Задача 4: Мониторинг системы**\n",
    "\n",
    "Создайте систему мониторинга используя sys и os:\n",
    "\n",
    "1. Создайте namedtuple `SystemInfo` с полями: cpu_count, memory_usage, process_id, user_name\n",
    "2. Используйте `os.cpu_count()` для получения количества процессоров\n",
    "3. Используйте `sys.getallocatedblocks()` для мониторинга памяти\n",
    "4. Используйте `os.getpid()` и `os.getlogin()` для информации о процессе\n",
    "5. Используйте `deque` для хранения последних 20 измерений\n",
    "6. Используйте `Counter` для подсчета частоты использования различных функций\n",
    "7. Используйте `defaultdict(list)` для группировки измерений по времени\n",
    "8. Сохраните историю мониторинга в pickle файл\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e9c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "# 1) SystemInfo namedtuple\n",
    "SystemInfo = namedtuple('SystemInfo', ['cpu_count', 'memory_usage', 'process_id', 'user_name'])\n",
    "\n",
    "# 2–4) Gathering statistics (system information)\n",
    "cpu_count = os.cpu_count()\n",
    "memory_usage = sys.getallocatedblocks()\n",
    "process_id = os.getpid()\n",
    "user_name = os.getlogin()\n",
    "\n",
    "current_info = SystemInfo(cpu_count, memory_usage, process_id, user_name)\n",
    "\n",
    "# 5) Create deque for containing last 20 statistics\n",
    "history: deque = deque(maxlen=20)\n",
    "history.append(current_info)\n",
    "\n",
    "for _ in range(5):\n",
    "    info = SystemInfo(\n",
    "        cpu_count=os.cpu_count(),\n",
    "        memory_usage=sys.getallocatedblocks(),\n",
    "        process_id=os.getpid(),\n",
    "        user_name=os.getlogin()\n",
    "    )\n",
    "    history.append(info)\n",
    "\n",
    "# 6) Count function calls (dummy imitation)\n",
    "function_calls = Counter()\n",
    "function_calls['collect_metrics'] += 1\n",
    "function_calls['save_data'] += 1\n",
    "function_calls['collect_metrics'] += 1\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# 7) Create defaultdict(list) for grouping statistics by time\n",
    "measurements_by_time = defaultdict(list)\n",
    "timestamp = int(time.time()) # one and only key (current timestamp)\n",
    "for info in history:\n",
    "    measurements_by_time[timestamp].append(info)\n",
    "\n",
    "\n",
    "# 8) Serialize using pickle\n",
    "with open('monitoring_history.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'history': history,\n",
    "        'function_calls': function_calls,\n",
    "        'measurements_by_time': measurements_by_time,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8337bc9",
   "metadata": {},
   "source": [
    "**Задача 5: Система логирования**\n",
    "\n",
    "Создайте систему логирования используя все изученные коллекции:\n",
    "\n",
    "1. Создайте namedtuple `LogEntry` с полями: timestamp, level, message, module, function\n",
    "2. Используйте `deque` для хранения последних 100 логов (кольцевой буфер)\n",
    "3. Используйте `defaultdict(list)` для группировки логов по уровням (DEBUG, INFO, WARNING, ERROR)\n",
    "4. Используйте `Counter` для подсчета количества логов каждого уровня\n",
    "5. Используйте `OrderedDict` для хранения логов по времени (FIFO)\n",
    "6. Используйте `ChainMap` для объединения различных источников логов\n",
    "7. Используйте `os.path` для работы с файлами логов\n",
    "8. Сериализуйте логи в JSON и pickle форматы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, defaultdict, Counter, OrderedDict, ChainMap\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# 1) LogEntry namedtuple\n",
    "LogEntry = namedtuple('LogEntry', ['timestamp', 'level', 'message', 'module', 'function'])\n",
    "\n",
    "# 2) Create deque with max length = 100\n",
    "recent_logs: deque = deque(maxlen=100)\n",
    "\n",
    "# Create several log examples\n",
    "now = time.time()\n",
    "entries = [\n",
    "    LogEntry(now, 'INFO', 'App started', 'main', 'run'),\n",
    "    LogEntry(now + 1, 'DEBUG', 'Config loaded', 'config', 'load'),\n",
    "    LogEntry(now + 2, 'WARNING', 'Low memory', 'system', 'check_resources'),\n",
    "    LogEntry(now + 3, 'ERROR', 'File not found', 'io', 'read_file'),\n",
    "    LogEntry(now + 4, 'INFO', 'User logged in', 'auth', 'login'),\n",
    "]\n",
    "\n",
    "for entry in entries:\n",
    "    recent_logs.append(entry)\n",
    "\n",
    "# 3) Grouping logs by level in defaultdict()\n",
    "logs_by_level = defaultdict(list)\n",
    "for log in recent_logs:\n",
    "    logs_by_level[log.level].append(log)\n",
    "\n",
    "# 4) Count logs\n",
    "level_counter = Counter(log.level for log in recent_logs)\n",
    "\n",
    "# 5) OrderedDict by time (FIFO)\n",
    "logs_by_time = OrderedDict()\n",
    "for log in recent_logs:\n",
    "    logs_by_time[log.timestamp] = log\n",
    "\n",
    "# ChainMap (logs union)\n",
    "archive_logs = { # dummy archive logs\n",
    "    now - 100: LogEntry(now - 100, 'INFO', 'Old event', 'legacy', 'old_func')\n",
    "}\n",
    "current_logs_dict = {log.timestamp: log for log in recent_logs}\n",
    "combined_logs = ChainMap(current_logs_dict, archive_logs)\n",
    "\n",
    "# 7) Make a directory for logs and prepare paths\n",
    "log_dir = os.path.join('logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "json_path = os.path.join(log_dir, 'app.log.json')\n",
    "pkl_path = os.path.join(log_dir, 'app.log.pkl')\n",
    "\n",
    "# 8) Serialization\n",
    "\n",
    "# Prepare json dictionary\n",
    "json_data = {\n",
    "    'recent_logs': [e._asdict() for e in recent_logs],\n",
    "    'logs_by_level': {\n",
    "        level: [e._asdict() for e in logs]\n",
    "        for level, logs in logs_by_level.items()\n",
    "    },\n",
    "    'level_counts': dict(level_counter),\n",
    "    'logs_by_time': {\n",
    "        str(ts): e._asdict()\n",
    "        for ts, e in logs_by_time.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'recent_logs': recent_logs,\n",
    "        'logs_by_level': logs_by_level,\n",
    "        'level_counter': level_counter,\n",
    "        'logs_by_time': logs_by_time,\n",
    "        'combined_logs': combined_logs,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f3d31",
   "metadata": {},
   "source": [
    "**Задача 6: Кэш-система**\n",
    "\n",
    "Создайте простую кэш-систему используя collections:\n",
    "\n",
    "1. Создайте namedtuple `CacheEntry` с полями: key, value, timestamp, access_count\n",
    "2. Используйте `OrderedDict` для реализации LRU (Least Recently Used) кэша\n",
    "3. Используйте `deque` для хранения истории доступа к ключам\n",
    "4. Используйте `Counter` для подсчета частоты доступа к каждому ключу\n",
    "5. Используйте `defaultdict(int)` для хранения счетчиков доступа\n",
    "6. Реализуйте методы: get, set, delete, clear, size\n",
    "7. Используйте `sys.getsizeof()` для мониторинга размера кэша\n",
    "8. Сериализуйте кэш в pickle формат для сохранения между сессиями\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6233daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, OrderedDict, deque, Counter, defaultdict\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# 1) CacheEntry\n",
    "CacheEntry = namedtuple('CacheEntry', ['key', 'value', 'timestamp', 'access_count'])\n",
    "\n",
    "# 2–6)\n",
    "class SimpleLRUCache:\n",
    "    def __init__(self, maxsize=10):\n",
    "        self.maxsize = maxsize\n",
    "        self.cache: OrderedDict = OrderedDict()         # LRU (using OrderedDict)\n",
    "        self.access_history: deque = deque(maxlen=100)  # 3) key access history (using deque)\n",
    "        self.access_counter: Counter = Counter()        # 4) count accesses \n",
    "        self.counters: defaultdict = defaultdict(int)   # 5) store accesses in defaultdict(int)\n",
    "\n",
    "    def get(self, key):\n",
    "        \"\"\"get() triggers access by existing key\"\"\"\n",
    "        if key in self.cache:\n",
    "            entry = self.cache.pop(key)\n",
    "            # Update access count access_count\n",
    "            new_count = entry.access_count + 1\n",
    "            new_entry = CacheEntry(key, entry.value, entry.timestamp, new_count)\n",
    "            self.cache[key] = new_entry\n",
    "            # Update history and counters\n",
    "            self.access_history.append(key)\n",
    "            self.access_counter[key] += 1\n",
    "            self.counters[key] += 1\n",
    "            return new_entry.value\n",
    "        return None\n",
    "\n",
    "    def set(self, key, value):\n",
    "        \"\"\"set() triggers modification by key\"\"\"\n",
    "        if key in self.cache:\n",
    "            # Update existing value\n",
    "            old = self.cache.pop(key)\n",
    "            new_entry = CacheEntry(key, value, time.time(), old.access_count)\n",
    "        else:\n",
    "            # Create new entry (key is not in cache)\n",
    "            new_entry = CacheEntry(key, value, time.time(), 0)\n",
    "            if len(self.cache) >= self.maxsize:\n",
    "                self.cache.popitem(last=False)  # pop LRU\n",
    "        self.cache[key] = new_entry\n",
    "\n",
    "    def delete(self, key):\n",
    "        if key in self.cache:\n",
    "            del self.cache[key]\n",
    "\n",
    "    def clear(self):\n",
    "        self.cache.clear()\n",
    "        self.access_history.clear()\n",
    "        self.access_counter.clear()\n",
    "        self.counters.clear()\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.cache)\n",
    "\n",
    "    def get_cache_size_bytes(self):\n",
    "        return sys.getsizeof(self.cache)  # 7) getter function returns size of cache\n",
    "\n",
    "### Code usage\n",
    "\n",
    "cache = SimpleLRUCache(maxsize=3)\n",
    "cache.set(\"a\", 1)\n",
    "cache.set(\"b\", 2)\n",
    "cache.set(\"c\", 3)\n",
    "cache.get(\"a\")     # second access to \"b\"\n",
    "cache.set(\"d\", 4)  # pop \"b\" (LRU value)\n",
    "\n",
    "# 8) Pickle serialization\n",
    "with open('cache.pkl', 'wb') as f:\n",
    "    pickle.dump(cache, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4e21f",
   "metadata": {},
   "source": [
    "**Задача 7: Анализатор текста**\n",
    "\n",
    "Создайте анализатор текста используя collections:\n",
    "\n",
    "1. Создайте namedtuple `WordInfo` с полями: word, frequency, length, first_occurrence\n",
    "2. Используйте `Counter` для подсчета частоты слов\n",
    "3. Используйте `defaultdict(list)` для группировки слов по длине\n",
    "4. Используйте `deque` для хранения последних 50 уникальных слов\n",
    "5. Используйте `OrderedDict` для хранения слов в порядке первого появления\n",
    "6. Используйте `os.path` для работы с текстовыми файлами\n",
    "7. Используйте `sys.getsizeof()` для анализа памяти\n",
    "8. Сохраните результаты анализа в JSON файл\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb47b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter, defaultdict, deque, OrderedDict\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# 1) Create namedtuple WordInfo\n",
    "WordInfo = namedtuple('WordInfo', ['word', 'frequency', 'length', 'first_occurrence'])\n",
    "\n",
    "# 6) Prepare path to text file\n",
    "text_file = os.path.join('data', 'sample.txt')\n",
    "os.makedirs(os.path.dirname(text_file), exist_ok=True)\n",
    "\n",
    "# Create text file (if not created yet) and write sample text\n",
    "if not os.path.exists(text_file):\n",
    "    sample_text = \"hello world hello python world python is great\"\n",
    "    with open(text_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(sample_text)\n",
    "\n",
    "# Retrieve text from file\n",
    "with open(text_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "words = text.split()\n",
    "\n",
    "# 5) OrderedDict for first occurrence\n",
    "first_occurrence = OrderedDict()\n",
    "for i, word in enumerate(words):\n",
    "    if word not in first_occurrence:\n",
    "        first_occurrence[word] = i\n",
    "\n",
    "# 2) Counter for word frequency\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# 3) Grouping by length\n",
    "words_by_length = defaultdict(list)\n",
    "for word in word_freq:\n",
    "    words_by_length[len(word)].append(word)\n",
    "\n",
    "# 4) deque for last 50 unique words (in order of appearance)\n",
    "unique_words_deque = deque(maxlen=50)\n",
    "for word in first_occurrence:  # already in order of appearance\n",
    "    unique_words_deque.append(word)\n",
    "\n",
    "# Collect WordInfo for each unique word\n",
    "word_infos = []\n",
    "for word in first_occurrence:\n",
    "    info = WordInfo(\n",
    "        word=word,\n",
    "        frequency=word_freq[word],\n",
    "        length=len(word),\n",
    "        first_occurrence=first_occurrence[word]\n",
    "    )\n",
    "    word_infos.append(info)\n",
    "\n",
    "# 7) Memory analysis\n",
    "memory_usage = {\n",
    "    'word_freq': sys.getsizeof(word_freq),\n",
    "    'first_occurrence': sys.getsizeof(first_occurrence),\n",
    "    'words_by_length': sys.getsizeof(words_by_length),\n",
    "    'unique_words_deque': sys.getsizeof(unique_words_deque),\n",
    "}\n",
    "\n",
    "# 8) Save to JSON\n",
    "json_data = {\n",
    "    'word_infos': [w._asdict() for w in word_infos],\n",
    "    'words_by_length': {str(k): v for k, v in words_by_length.items()},  # keys in JSON are strings\n",
    "    'last_unique_words': list(unique_words_deque),\n",
    "    'memory_usage_bytes': memory_usage\n",
    "}\n",
    "\n",
    "output_path = os.path.join('data', 'analysis.json')\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442fa22",
   "metadata": {},
   "source": [
    "**Задача 8: Система управления задачами**\n",
    "\n",
    "Создайте систему управления задачами (TODO) используя все изученные концепции:\n",
    "\n",
    "1. Создайте namedtuple `Task` с полями: id, title, description, priority, status, created_date\n",
    "2. Используйте `defaultdict(list)` для группировки задач по статусу (todo, in_progress, done)\n",
    "3. Используйте `deque` для очереди задач с высоким приоритетом\n",
    "4. Используйте `Counter` для подсчета задач по приоритету\n",
    "5. Используйте `OrderedDict` для хранения задач в порядке создания\n",
    "6. Используйте `ChainMap` для объединения различных списков задач\n",
    "7. Используйте `os.path` для работы с файлами задач\n",
    "8. Реализуйте функции: add_task, complete_task, get_tasks_by_status, get_priority_queue\n",
    "9. Сериализуйте все данные в JSON и pickle форматы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque, Counter, OrderedDict, ChainMap\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# 1) Create namedtuple Task\n",
    "Task = namedtuple('Task', ['id', 'title', 'description', 'priority', 'status', 'created_date'])\n",
    "\n",
    "# Tasks storage\n",
    "tasks_by_status = defaultdict(list)  # 2) grouping by status\n",
    "task_queue_high = deque()            # 3) high priority queue\n",
    "all_tasks_ordered = OrderedDict()    # 5) order of creation\n",
    "\n",
    "# Example tasks\n",
    "now = time.time()\n",
    "sample_tasks = [\n",
    "    Task(1, \"Fix bug\", \"Fix login bug\", \"high\", \"todo\", now),\n",
    "    Task(2, \"Write docs\", \"API documentation\", \"medium\", \"in_progress\", now + 1),\n",
    "    Task(3, \"Test feature\", \"Run tests\", \"high\", \"todo\", now + 2),\n",
    "    Task(4, \"Deploy\", \"Deploy to prod\", \"critical\", \"todo\", now + 3),\n",
    "]\n",
    "\n",
    "# Fill structures\n",
    "for task in sample_tasks:\n",
    "    tasks_by_status[task.status].append(task)\n",
    "    all_tasks_ordered[task.id] = task\n",
    "    if task.priority in (\"high\", \"critical\"):\n",
    "        task_queue_high.append(task)\n",
    "\n",
    "# 4) Counter for priority\n",
    "priority_counter = Counter(task.priority for task in all_tasks_ordered.values())\n",
    "\n",
    "# 6) ChainMap — combine different sources (e.g. main + archive)\n",
    "archived_tasks = {\n",
    "    0: Task(0, \"Old task\", \"Done long ago\", \"low\", \"done\", now - 1000)\n",
    "}\n",
    "current_tasks_dict = {tid: task for tid, task in all_tasks_ordered.items()}\n",
    "all_tasks_chain = ChainMap(current_tasks_dict, archived_tasks)\n",
    "\n",
    "# 7) Paths to files\n",
    "data_dir = os.path.join('todo_data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# 8) Functions (simple, without validations)\n",
    "def add_task(task_id, title, desc, priority, status=\"todo\"):\n",
    "    new_task = Task(task_id, title, desc, priority, status, time.time())\n",
    "    tasks_by_status[status].append(new_task)\n",
    "    all_tasks_ordered[task_id] = new_task\n",
    "    if priority in (\"high\", \"critical\"):\n",
    "        task_queue_high.append(new_task)\n",
    "    priority_counter[priority] += 1\n",
    "\n",
    "def complete_task(task_id):\n",
    "    if task_id in all_tasks_ordered:\n",
    "        old_task = all_tasks_ordered[task_id]\n",
    "        # Remove from old status\n",
    "        tasks_by_status[old_task.status].remove(old_task)\n",
    "        # Create updated task\n",
    "        new_task = old_task._replace(status=\"done\")\n",
    "        tasks_by_status[\"done\"].append(new_task)\n",
    "        all_tasks_ordered[task_id] = new_task\n",
    "        # Update priority counter (priority does not change)\n",
    "        # High priority queue is not cleared — just keep as history\n",
    "\n",
    "def get_tasks_by_status(status):\n",
    "    return tasks_by_status[status]\n",
    "\n",
    "def get_priority_queue():\n",
    "    return list(task_queue_high)\n",
    "\n",
    "### Code usage\n",
    "\n",
    "\n",
    "add_task(5, \"Review PR\", \"Code review\", \"high\")\n",
    "complete_task(1)\n",
    "\n",
    "# 9) Serialization\n",
    "\n",
    "# JSON\n",
    "json_data = {\n",
    "    'tasks_by_status': {\n",
    "        status: [t._asdict() for t in tasks]\n",
    "        for status, tasks in tasks_by_status.items()\n",
    "    },\n",
    "    'priority_counter': dict(priority_counter),\n",
    "    'all_tasks_ordered': {str(k): v._asdict() for k, v in all_tasks_ordered.items()},\n",
    "    'priority_queue': [t._asdict() for t in task_queue_high],\n",
    "}\n",
    "\n",
    "json_path = os.path.join(data_dir, 'tasks.json')\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "# Pickle\n",
    "pkl_path = os.path.join(data_dir, 'tasks.pkl')\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'tasks_by_status': tasks_by_status,\n",
    "        'task_queue_high': task_queue_high,\n",
    "        'priority_counter': priority_counter,\n",
    "        'all_tasks_ordered': all_tasks_ordered,\n",
    "        'all_tasks_chain': all_tasks_chain,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78697d",
   "metadata": {},
   "source": [
    "**Задача 9: Система мониторинга производительности**\n",
    "\n",
    "Создайте систему мониторинга производительности используя sys и collections:\n",
    "\n",
    "1. Создайте namedtuple `PerformanceMetric` с полями: function_name, execution_time, memory_usage, timestamp\n",
    "2. Используйте `deque` для хранения последних 100 измерений производительности\n",
    "3. Используйте `defaultdict(list)` для группировки метрик по функциям\n",
    "4. Используйте `Counter` для подсчета количества вызовов каждой функции\n",
    "5. Используйте `OrderedDict` для хранения метрик в хронологическом порядке\n",
    "6. Используйте `sys.getsizeof()` для мониторинга памяти\n",
    "7. Используйте `os.path` для работы с файлами метрик\n",
    "8. Реализуйте функции: record_metric, get_function_stats, get_memory_usage, export_metrics\n",
    "9. Сериализуйте метрики в JSON и pickle форматы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701382fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, defaultdict, Counter, OrderedDict\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# 1) Create PerformanceMetric (namedtuple)\n",
    "PerformanceMetric = namedtuple('PerformanceMetric', ['function_name', 'execution_time', 'memory_usage', 'timestamp'])\n",
    "\n",
    "# Metrics storage\n",
    "metrics_deque = deque(maxlen=100)           # 2) last 100 measurements\n",
    "metrics_by_function = defaultdict(list)     # 3) by functions\n",
    "call_counter = Counter()                    # 4) number of calls\n",
    "metrics_chrono = OrderedDict()              # 5) chronological order\n",
    "\n",
    "# Metrics examples\n",
    "now = time.time()\n",
    "sample_metrics = [\n",
    "    PerformanceMetric(\"load_data\", 0.12, sys.getsizeof([]), now),\n",
    "    PerformanceMetric(\"process\", 0.45, sys.getsizeof({1:2}), now + 1),\n",
    "    PerformanceMetric(\"load_data\", 0.10, sys.getsizeof([1,2,3]), now + 2),\n",
    "    PerformanceMetric(\"save_result\", 0.08, sys.getsizeof(\"ok\"), now + 3),\n",
    "]\n",
    "\n",
    "# Fill collections\n",
    "for metric in sample_metrics:\n",
    "    metrics_deque.append(metric)\n",
    "    metrics_by_function[metric.function_name].append(metric)\n",
    "    call_counter[metric.function_name] += 1\n",
    "    metrics_chrono[metric.timestamp] = metric\n",
    "\n",
    "# 7) Prepare path to files\n",
    "metrics_dir = os.path.join('metrics')\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "# 8) Functions\n",
    "def record_metric(func_name, exec_time):\n",
    "    mem = sys.getsizeof([])  # just an example of memory\n",
    "    ts = time.time()\n",
    "    metric = PerformanceMetric(func_name, exec_time, mem, ts)\n",
    "    metrics_deque.append(metric)\n",
    "    metrics_by_function[func_name].append(metric)\n",
    "    call_counter[func_name] += 1\n",
    "    metrics_chrono[ts] = metric\n",
    "\n",
    "def get_function_stats(func_name):\n",
    "    return metrics_by_function.get(func_name, [])\n",
    "\n",
    "def get_memory_usage():\n",
    "    return sys.getsizeof(metrics_deque) + sys.getsizeof(metrics_by_function)\n",
    "\n",
    "def export_metrics():\n",
    "    # JSON\n",
    "    json_data = {\n",
    "        'recent_metrics': [m._asdict() for m in metrics_deque],\n",
    "        'by_function': {\n",
    "            fn: [m._asdict() for m in metrics]\n",
    "            for fn, metrics in metrics_by_function.items()\n",
    "        },\n",
    "        'call_counts': dict(call_counter),\n",
    "    }\n",
    "    json_path = os.path.join(metrics_dir, 'performance.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    # Pickle\n",
    "    pkl_path = os.path.join(metrics_dir, 'performance.pkl')\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'metrics_deque': metrics_deque,\n",
    "            'metrics_by_function': metrics_by_function,\n",
    "            'call_counter': call_counter,\n",
    "            'metrics_chrono': metrics_chrono,\n",
    "        }, f)\n",
    "\n",
    "\n",
    "### Code usage\n",
    "\n",
    "record_metric(\"validate\", 0.05)\n",
    "export_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a0615",
   "metadata": {},
   "source": [
    "**Задача 10: Комплексная система управления данными**\n",
    "\n",
    "Создайте комплексную систему управления данными, объединяющую все изученные концепции:\n",
    "\n",
    "1. Создайте несколько namedtuple для различных типов данных (User, Product, Order, etc.)\n",
    "2. Используйте `defaultdict` для создания индексов по различным полям\n",
    "3. Используйте `deque` для реализации очередей обработки данных\n",
    "4. Используйте `Counter` для аналитики и статистики\n",
    "5. Используйте `OrderedDict` для хранения данных в определенном порядке\n",
    "6. Используйте `ChainMap` для объединения различных источников данных\n",
    "7. Используйте `os` и `sys` для работы с файловой системой и мониторинга\n",
    "8. Реализуйте CRUD операции (Create, Read, Update, Delete)\n",
    "9. Добавьте функции экспорта/импорта данных в различных форматах\n",
    "10. Сериализуйте все данные в JSON, pickle и другие форматы\n",
    "11. Добавьте типизацию для всех функций и классов\n",
    "12. Реализуйте систему логирования для отслеживания операций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque, Counter, OrderedDict, ChainMap\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "### Logging — through simple list log_entries and log_op function ###\n",
    "\n",
    "# 1) namedtuple for different type of data\n",
    "User = namedtuple('User', ['id', 'name', 'email'])\n",
    "Product = namedtuple('Product', ['id', 'name', 'price'])\n",
    "Order = namedtuple('Order', ['id', 'user_id', 'product_id', 'status'])\n",
    "\n",
    "# Example data\n",
    "users = [\n",
    "    User(1, \"Alice\", \"alice@example.com\"),\n",
    "    User(2, \"Bob\", \"bob@example.com\")\n",
    "]\n",
    "\n",
    "products = [\n",
    "    Product(101, \"Laptop\", 999.99),\n",
    "    Product(102, \"Mouse\", 19.99)\n",
    "]\n",
    "\n",
    "orders = [\n",
    "    Order(1001, 1, 101, \"shipped\"),\n",
    "    Order(1002, 2, 102, \"pending\")\n",
    "]\n",
    "\n",
    "# 2) Indexes (for various fields) through defaultdict\n",
    "user_index: Dict[int, User] = {u.id: u for u in users}\n",
    "product_index: Dict[int, Product] = {p.id: p for p in products}\n",
    "orders_by_user = defaultdict(list)\n",
    "for order in orders:\n",
    "    orders_by_user[order.user_id].append(order)\n",
    "\n",
    "# 3) deque for processing orders queue\n",
    "processing_queue: deque = deque()\n",
    "for order in orders:\n",
    "    if order.status == \"pending\":\n",
    "        processing_queue.append(order)\n",
    "\n",
    "# 4) Create counter for analysis of product and status\n",
    "product_counter = Counter(order.product_id for order in orders)\n",
    "status_counter = Counter(order.status for order in orders)\n",
    "\n",
    "# 5) Store in chronological order\n",
    "all_orders_ordered: OrderedDict = OrderedDict()\n",
    "for order in orders:\n",
    "    all_orders_ordered[order.id] = order\n",
    "\n",
    "# 6) ChainMap — combine sources (e.g. main + dummy archive)\n",
    "archived_orders = {999: Order(999, 1, 102, \"delivered\")}\n",
    "current_orders = {o.id: o for o in orders}\n",
    "orders_chain = ChainMap(current_orders, archived_orders)\n",
    "\n",
    "# Simple logging (list of records)\n",
    "log_entries: List[str] = []\n",
    "\n",
    "def log_op(operation: str, details: str):\n",
    "    log_entries.append(f\"[{operation}] {details}\")\n",
    "\n",
    "# 7) Paths and monitoring\n",
    "data_dir = os.path.join(\"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "def get_memory_usage() -> int:\n",
    "    return sys.getsizeof(users) + sys.getsizeof(orders) + sys.getsizeof(products)\n",
    "\n",
    "# 8) CRUD operations\n",
    "\n",
    "def create_user(user_id: int, name: str, email: str):\n",
    "    user = User(user_id, name, email)\n",
    "    users.append(user)\n",
    "    user_index[user_id] = user\n",
    "    log_op(\"CREATE\", f\"User {user_id}\")\n",
    "\n",
    "def get_user(user_id: int) -> Optional[User]:\n",
    "    return user_index.get(user_id)\n",
    "\n",
    "def update_user(user_id: int, name: str = None, email: str = None):\n",
    "    if user_id in user_index:\n",
    "        old = user_index[user_id]\n",
    "        new = User(old.id, name or old.name, email or old.email)\n",
    "        # Обновляем в списке и индексе\n",
    "        for i, u in enumerate(users):\n",
    "            if u.id == user_id:\n",
    "                users[i] = new\n",
    "                break\n",
    "        user_index[user_id] = new\n",
    "        log_op(\"UPDATE\", f\"User {user_id}\")\n",
    "\n",
    "def delete_user(user_id: int):\n",
    "    if user_id in user_index:\n",
    "        users[:] = [u for u in users if u.id != user_id]\n",
    "        del user_index[user_id]\n",
    "        log_op(\"DELETE\", f\"User {user_id}\")\n",
    "\n",
    "# 9–10)\n",
    "\n",
    "# Export data to JSON and pickle\n",
    "def export_data():\n",
    "    # JSON\n",
    "    json_data = {\n",
    "        \"users\": [u._asdict() for u in users],\n",
    "        \"products\": [p._asdict() for p in products],\n",
    "        \"orders\": [o._asdict() for o in orders],\n",
    "        \"analytics\": {\n",
    "            \"product_counts\": dict(product_counter),\n",
    "            \"status_counts\": dict(status_counter)\n",
    "        },\n",
    "        \"logs\": log_entries\n",
    "    }\n",
    "    with open(os.path.join(data_dir, \"data.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    # Pickle\n",
    "    with open(os.path.join(data_dir, \"data.pkl\"), \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"users\": users,\n",
    "            \"products\": products,\n",
    "            \"orders\": orders,\n",
    "            \"user_index\": user_index,\n",
    "            \"product_index\": product_index,\n",
    "            \"orders_by_user\": orders_by_user,\n",
    "            \"processing_queue\": processing_queue,\n",
    "            \"log_entries\": log_entries\n",
    "        }, f)\n",
    "\n",
    "# Import data from pickle\n",
    "def import_data_from_pickle():\n",
    "    path = os.path.join(data_dir, \"data.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        # Update global variables\n",
    "        global users, products, orders, user_index, product_index, orders_by_user, processing_queue, log_entries\n",
    "        users = data[\"users\"]\n",
    "        products = data[\"products\"]\n",
    "        orders = data[\"orders\"]\n",
    "        user_index = data[\"user_index\"]\n",
    "        product_index = data[\"product_index\"]\n",
    "        orders_by_user = data[\"orders_by_user\"]\n",
    "        processing_queue = data[\"processing_queue\"]\n",
    "        log_entries = data[\"log_entries\"]\n",
    "\n",
    "\n",
    "### Code usage\n",
    "\n",
    "create_user(3, \"Charlie\", \"charlie@example.com\")\n",
    "update_user(1, email=\"alice.new@example.com\")\n",
    "delete_user(2)\n",
    "export_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
